# -*- coding: utf-8 -*-
"""Book Recommender System

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BG7wKt3dWYWt8BGf2LZuzyJMtSjzegI4

<h1>Mounting to drive</h1>
"""

#Connecting to drive
from google.colab import drive
drive.mount('/content/drive')

"""<h1>Importing Libraries<h1>"""

#Importing some common libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""<h1>Reading the dataset and preprocessing </h1>

"""

#Reading the books dataset
books = pd.read_csv('/content/drive/MyDrive/Book_Recommender_System/Dataset/Books.csv',  on_bad_lines='skip', encoding='latin-1')

books.head(5)

books.shape

users = pd.read_csv('/content/drive/MyDrive/Book_Recommender_System/Dataset/Users.csv', on_bad_lines='skip', encoding='latin-1')

users.head(5)

users.shape

ratings = pd.read_csv('/content/drive/MyDrive/Book_Recommender_System/Dataset/Ratings.csv', on_bad_lines='skip', encoding='latin-1')

ratings.head(5)

ratings.shape

#Counting the number of ratings given by each user
ratings['User-ID'].value_counts()

"""<h3>Here we can see that some of the users have only provided 1 rating. We can not cluster such users as 1 rating is not sufficient to find out about their taste. Those users who have rated less than 100 books should be filtered out since their information will not be very effective.</h3>"""

#Number of users who have rated more than 100 books
effective_users = ratings['User-ID'].value_counts() > 100

effective_users[effective_users].shape

y = effective_users[effective_users].index

y

ratings = ratings[ratings['User-ID'].isin(y)]

ratings.shape

BookwithRatings = ratings.merge(books, on="ISBN")

BookwithRatings.head(5)

#Dropping uncessary columns
BookwithRatings=BookwithRatings.drop(['Image-URL-S','Image-URL-M', 'Image-URL-L'], axis=1)
BookwithRatings.head(5)

BookwithRatings.shape

"""<h3>We got 604854 data after merging rating with book information.</h3>"""

num_rating = BookwithRatings.groupby('Book-Title')['Book-Rating'].count().reset_index()

num_rating.rename(columns = {'Book-Rating' : 'No_of_ratings'}, inplace = True)

num_rating.head()

"""<h3>We can see that some of the books got only 1-3 ratings. These type of data needs to be removed as well otherwise there will be imbalance. We will keep only those books which has atleast 50 ratings.</h3>"""

BookwithRatings = BookwithRatings.merge(num_rating, on='Book-Title')

BookwithRatings = BookwithRatings[BookwithRatings['No_of_ratings']>=50]

BookwithRatings.sample(10)

BookwithRatings.shape

BookwithRatings.drop_duplicates(['User-ID','Book-Title'], inplace=True)

BookwithRatings.shape

"""<h3>Finally we have 97962 data</h3>"""

pivot_table = BookwithRatings.pivot_table(columns='User-ID', index = 'Book-Title', values = 'Book-Rating')

pivot_table.shape

pivot_table.fillna(0, inplace=True)

pivot_table

#WE CAN USE CSR MATRIX TO ENSURE 0 WON'T BE CONSIDERED

from scipy.sparse import csr_matrix

final_data = csr_matrix(pivot_table)

final_data

from sklearn.neighbors import NearestNeighbors
model = NearestNeighbors(algorithm = 'brute')

model.fit(final_data)

book_names = pivot_table.index

import pickle
pickle.dump(model, open('/content/drive/MyDrive/Book_Recommender_System/SavedModel/model.pkl', 'wb'))
pickle.dump(book_names, open('/content/drive/MyDrive/Book_Recommender_System/SavedModel/books_name.pkl', 'wb'))
pickle.dump(final_data, open('/content/drive/MyDrive/Book_Recommender_System/SavedModel/final_data.pkl', 'wb'))
pickle.dump(pivot_table, open('/content/drive/MyDrive/Book_Recommender_System/SavedModel/pivot_table.pkl', 'wb'))

def recommend_book(book_name):
  book_id = np.where(pivot_table.index == book_name)[0][0]
  distance, suggestion = model.kneighbors(pivot_table.iloc[book_id,:].values.reshape(1,-1), n_neighbors=5)
  for i in range(len(suggestion)):
    books = pivot_table.index[suggestion[i]]
    for j in books:
      print(j)

book_name = '1984'
recommend_book(book_name)

# Predict the cluster label for each book
book_cluster_labels = kmeans.predict(pivot_table)

from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances

# Create a KMeans object with 5 clusters
kmeans = KMeans(n_clusters=5)

# Fit the KMeans object to the pivot table data
kmeans.fit(pivot_table)

# Predict the cluster label for each book
book_cluster_labels = kmeans.predict(pivot_table)

# Identify the books in the same cluster as the book that the user is interested in
def recommend_book_kmeans(book_name, num_recommendations=5):
  if book_name in pivot_table.index:
    book_id = np.where(pivot_table.index == book_name)[0][0]
    book_cluster_label = book_cluster_labels[book_id]

    # Find all books in the same cluster
    cluster_books = pivot_table.index[book_cluster_labels == book_cluster_label]

    # Calculate the similarity between the selected book and others in the same cluster
    similarities = euclidean_distances(pivot_table.values[book_id].reshape(1, -1), pivot_table.values[book_cluster_labels == book_cluster_label])

    # Sort books by similarity and recommend the top 5 (excluding the book itself)
    recommended_indices = np.argsort(similarities)[0][1:num_recommendations + 1]

    # Recommend the books in the same cluster to the user
    print(f"Recommended books for '{book_name}':")
    for idx in recommended_indices:
      print(cluster_books[idx])
  else:
    print(f"Book '{book_name}' not found in the dataset.")

# Get the book name that the user is interested in
book_name = input('Enter the book name that you are interested in: ')

# Recommend the top 5 most similar books using K-Means clustering
recommend_book_kmeans(book_name)